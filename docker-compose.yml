version: '3.8'

services:
  
  # 1. Zookeeper (Required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - bigdata-network

  # 2. Kafka 
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - bigdata-network

  # 3. Kafka UI 
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - bigdata-network

  # 4. Flight Producer 
  flight-producer:
    build: ./producer
    container_name: flight-producer
    depends_on:
      - kafka
    restart: always
    networks:
      - bigdata-network

  # 5. Spark Master (Official Apache Image)
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    hostname: spark-master
    # The official image runs as a 'CMD' by default, we override it to start the Master class
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8080" # Spark Master Web UI 
      - "7077:7077" # Spark Master Protocol Port
    networks:
      - bigdata-network
  
  # 6. Spark Worker (Official Apache Image)
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark-master
    # We explicitly tell the worker where the master is via command arguments
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    networks:
      - bigdata-network
  
  # 7. Airflow Init
  airflow-init:
    build: ./airflow
    container_name: airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: /init-airflow.sh
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - bigdata-network

  # 8. Airflow Webserver
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "saadan_saad"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    networks:
      - bigdata-network
    command: webserver

  # 9. Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "saadan_saad"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./dags:/opt/airflow/dags
    networks:
      - bigdata-network
    command: scheduler

  # 10. Postgres
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 11. MongoDB (Operational Data Store)
  mongodb:
    image: mongo:6.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_DATABASE=flight_tracking
    volumes:
      - mongodb_data:/data/db
    networks:
      - bigdata-network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  # 12. Redis (Caching Layer)
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # 13. HDFS Namenode
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=flight_cluster
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hdfs_namenode_data:/hadoop/dfs/name
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 14. HDFS Datanode
  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
    ports:
      - "9864:9864"
    volumes:
      - hdfs_datanode_data:/hadoop/dfs/data
    depends_on:
      - hdfs-namenode
    networks:
      - bigdata-network

  # 15. Hive Metastore (PostgreSQL backend)
  postgres-hive:
    image: postgres:13
    container_name: postgres-hive
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    networks:
      - bigdata-network
    volumes:
      - postgres_hive_data:/var/lib/postgresql/data

  # 16. Hive Metastore Service
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    environment:
      - SERVICE_NAME=hive-metastore
      - DB_DRIVER=postgres
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-hive:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive
    ports:
      - "9083:9083"
    depends_on:
      - postgres-hive
      - hdfs-namenode
    networks:
      - bigdata-network
    command: /opt/hive/bin/hive --service metastore

  # 17. Streamlit Dashboard
  streamlit-dashboard:
    build: ./dashboard
    container_name: streamlit-dashboard
    ports:
      - "8501:8501"
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres-hive:
        condition: service_started
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - HIVE_HOST=postgres-hive
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - bigdata-network
    volumes:
      - ./dashboard:/app
    restart: unless-stopped

volumes:
  mongodb_data:
  hdfs_namenode_data:
  hdfs_datanode_data:
  postgres_hive_data:

networks:
  bigdata-network:
    driver: bridge