version: '3.8'

services:
  
  # 1. Zookeeper (Required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - bigdata-network

  # 2. Kafka 
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - bigdata-network

  # 3. Kafka UI 
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - bigdata-network

  # 4. Flight Producer 
  flight-producer:
    build: ./producer
    container_name: flight-producer
    depends_on:
      - kafka
    restart: always
    networks:
      - bigdata-network

  # 5. Spark Master (Official Apache Image)
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    hostname: spark-master
    # The official image runs as a 'CMD' by default, we override it to start the Master class
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8080" # Spark Master Web UI 
      - "7077:7077" # Spark Master Protocol Port
    networks:
      - bigdata-network
  
  # 6. Spark Worker (Official Apache Image)
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark-master
    # We explicitly tell the worker where the master is via command arguments
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    networks:
      - bigdata-network
  
  # 7. Airflow Init
  airflow-init:
    build: ./airflow
    container_name: airflow-init
    command: version
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: >
      bash -c "airflow db init &&
      airflow users create --username admin --password admin --firstname Peter --lastname Parker --role Admin --email spiderman@superhero.org &&
      airflow db upgrade"
    depends_on:
      - postgres
    networks:
      - bigdata-network

  # 8. Airflow Webserver
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "saadan_saad"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    networks:
      - bigdata-network
    command: webserver

  # 9. Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "saadan_saad"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./dags:/opt/airflow/dags
    networks:
      - bigdata-network
    command: scheduler

  # 10. Postgres
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - bigdata-network

networks:
  bigdata-network:
    driver: bridge