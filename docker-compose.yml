services:
  
  # Zookeeper (Required for Kafka)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - bigdata-network

  # Kafka 
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 30s

  # Kafka UI 
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - bigdata-network

  # Flight Producer 
  flight-producer:
    build: ./producer
    container_name: flight-producer
    depends_on:
      kafka:
        condition: service_healthy
    restart: always
    networks:
      - bigdata-network

  # Spark Master (Official Apache Image, others were troublesome)
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    hostname: spark-master
    # The official image runs as a 'CMD' by default, we override it to start the Master class
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8080" # Spark Master Web UI 
      - "7077:7077" # Spark Master Protocol Port
    networks:
      - bigdata-network
  
  # Spark Worker 
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    depends_on:
      - spark-master
    # explicitly tell the worker where the master is via command arguments
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
    networks:
      - bigdata-network
  
  # Airflow Init
  airflow-init:
    build: ./airflow
    container_name: airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    entrypoint: /init-airflow.sh
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - bigdata-network

  # Airflow Webserver
  airflow-webserver:
    build: ./airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "saadan_saad"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    networks:
      - bigdata-network
    command: webserver

  # Airflow Scheduler
  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres/airflow"
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__SECRET_KEY: "saadan_saad"
      AIRFLOW_CONN_SPARK_DEFAULT: '{"conn_type": "spark", "host": "spark://spark-master", "port": 7077}'
    volumes:
      - ./dags:/opt/airflow/dags
    networks:
      - bigdata-network
    command: scheduler

  # Postgres
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # MongoDB (Operational Data Store)
  mongodb:
    image: mongo:6.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      - MONGO_INITDB_DATABASE=flight_tracking
    volumes:
      - mongodb_data:/data/db
    networks:
      - bigdata-network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis (Caching Layer)
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # HDFS Namenode
  hdfs-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-namenode
    environment:
      - CLUSTER_NAME=flight_cluster
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - hdfs_namenode_data:/hadoop/dfs/name
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  # HDFS Datanode
  hdfs-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hdfs-datanode
    environment:
      - SERVICE_PRECONDITION=hdfs-namenode:9870
    ports:
      - "9864:9864"
    volumes:
      - hdfs_datanode_data:/hadoop/dfs/data
    depends_on:
      - hdfs-namenode
    networks:
      - bigdata-network

  # Hive Metastore (PostgreSQL backend)
  postgres-hive:
    image: postgres:13
    container_name: postgres-hive
    environment:
      - POSTGRES_USER=hive
      - POSTGRES_PASSWORD=hive
      - POSTGRES_DB=metastore
    networks:
      - bigdata-network
    volumes:
      - postgres_hive_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Hive Metastore Service
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    environment:
      - SERVICE_NAME=hive-metastore
      - DB_DRIVER=postgres
      - SKIP_SCHEMA_INIT=false
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-hive:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive
      - HADOOP_CLIENT_OPTS=-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-hive:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive
    ports:
      - "9083:9083"
    depends_on:
      postgres-hive:
        condition: service_healthy
      hdfs-namenode:
        condition: service_started
    networks:
      - bigdata-network
    entrypoint:
      - /bin/bash
      - -c
      - |
        export HADOOP_CLIENT_OPTS="-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-hive:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive"
        echo "Waiting for PostgreSQL to be ready (10 seconds)..."
        sleep 10
        echo "Checking Hive schema status..."
        if /opt/hive/bin/schematool -dbType postgres -info 2>&1 | grep -q "schemaTool completed\|Hive distribution version"; then
          echo "Hive schema exists and is valid, starting metastore..."
        else
          echo "Schema not found or invalid, initializing with PostgreSQL..."
          /opt/hive/bin/schematool -dbType postgres -initSchema --verbose 2>&1 || (echo "ERROR: Schema initialization failed." && exit 1)
        fi
        echo "Starting Hive Metastore..."
        exec /opt/hive/bin/hive --service metastore

  # Streamlit Dashboard
  streamlit-dashboard:
    build: ./dashboard
    container_name: streamlit-dashboard
    ports:
      - "8501:8501"
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres-hive:
        condition: service_started
    environment:
      - MONGODB_URI=mongodb://mongodb:27017/
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - HIVE_HOST=postgres-hive
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - bigdata-network
    volumes:
      - ./dashboard:/app
    restart: unless-stopped

volumes:
  mongodb_data:
  hdfs_namenode_data:
  hdfs_datanode_data:
  postgres_hive_data:

networks:
  bigdata-network:
    driver: bridge